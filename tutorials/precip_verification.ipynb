{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial demonstrating verification of v1 precip against jra55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pyLatte package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatte import utils\n",
    "from pylatte import skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently, the following packages are required to load the data - this process will be replaced by the CAFE cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some plotting packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Jupyter specific -----\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note about the methodology of pyLatte\n",
    "The pyLatte package is constructed around the xarray Python package. This is particularly useful for verifications computations, which require large numbers of samples (different model runs) to converge. \n",
    "\n",
    "The approach here is to generate very large xarray objects that reference all data required for the verification, but do not store the data in memory. Operations are performed on these xarray objects out-of-memory. When it is necessary to perform a compute (e.g. to produce a plot), this is distributed over multiple processors using the dask Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise dask (currently not working on vm31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import distributed\n",
    "# client = distributed.Client(local_dir='/tmp/squ027-dask-worker-space', n_workers=4)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct xarray objects for forecasts and observations\n",
    "(The CAFE cookbook will replace these code blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling details -----\n",
    "resample_freq = 'M'\n",
    "resample_method = 'sum'\n",
    "\n",
    "# Location of forecast data -----\n",
    "fcst_folder = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/'\n",
    "fcst_filename = 'atmos_daily*'\n",
    "\n",
    "# Location of observation data -----\n",
    "obsv_folder = '/OSM/CBR/OA_DCFP/data/observations/jra55/isobaric/061_tprat/'\n",
    "obsv_filename = 'anl_surf125.002_prmsl.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dates (takes approximately 1 min 30 sec per date) -----\n",
    "init_dates = pd.date_range('2002-06','2005-06' , freq='1MS')\n",
    "\n",
    "# Ensembles to include -----\n",
    "ensembles = range(1,12)\n",
    "\n",
    "# Forecast length -----\n",
    "FCST_LENGTH = 2 # years\n",
    "lead_times = utils.get_lead_times(FCST_LENGTH, resample_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct forecasts xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data for forecast started on 6-2002...\n",
      "   Elapsed: 5.680202484130859 sec\n",
      "Gathering data for forecast started on 7-2002...\n",
      "   Elapsed: 12.016013145446777 sec\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many different dimensions to concatenate: {'lat', 'latb'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7dc122f46d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m                                                 \u001b[0;34m'/mn'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                 \u001b[0;34m'/OUTPUT.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                 '/' + fcst_filename, autoclose=True)\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0;31m# Concatenate along 'ensemble' dimension/coordinate -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mds_temp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ensemble'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, lock, data_vars, coords, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconcat_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_CONCAT_DIM_DEFAULT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             combined = auto_combine(datasets, compat=compat,\n\u001b[0;32m--> 549\u001b[0;31m                                     data_vars=data_vars, coords=coords)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             combined = auto_combine(datasets, concat_dim=concat_dim,\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/combine.py\u001b[0m in \u001b[0;36mauto_combine\u001b[0;34m(datasets, concat_dim, compat, data_vars, coords)\u001b[0m\n\u001b[1;32m    434\u001b[0m         concatenated = [_auto_concat(ds, dim=dim,\n\u001b[1;32m    435\u001b[0m                                      data_vars=data_vars, coords=coords)\n\u001b[0;32m--> 436\u001b[0;31m                         for ds in grouped]\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mconcatenated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/combine.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    434\u001b[0m         concatenated = [_auto_concat(ds, dim=dim,\n\u001b[1;32m    435\u001b[0m                                      data_vars=data_vars, coords=coords)\n\u001b[0;32m--> 436\u001b[0;31m                         for ds in grouped]\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mconcatenated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/combine.py\u001b[0m in \u001b[0;36m_auto_concat\u001b[0;34m(datasets, dim, data_vars, coords)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 raise ValueError('too many different dimensions to '\n\u001b[0;32m--> 359\u001b[0;31m                                  'concatenate: %s' % concat_dims)\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 raise ValueError('cannot infer dimension to concatenate: '\n",
      "\u001b[0;31mValueError\u001b[0m: too many different dimensions to concatenate: {'lat', 'latb'}"
     ]
    }
   ],
   "source": [
    "# import pdb, traceback, sys\n",
    "\n",
    "# ==================================================\n",
    "# Initialize xarray object for first init_date -----\n",
    "# ==================================================\n",
    "with utils.timer():\n",
    "    print(f'Gathering data for forecast started on {init_dates[0].month}-{init_dates[0].year}...')\n",
    "    \n",
    "    ds_fcst = xr.open_mfdataset(fcst_folder + \n",
    "                                '/yr' + str(init_dates[0].year) + \n",
    "                                '/mn' + str(init_dates[0].month) + \n",
    "                                '/OUTPUT.' + str(ensembles[0]) + \n",
    "                                '/' + fcst_filename, autoclose=True)\n",
    "    ds_fcst.coords['ensemble'] = ensembles[0]\n",
    "\n",
    "    for ensemble in ensembles[1:]:\n",
    "        ds_temp = xr.open_mfdataset(fcst_folder + \n",
    "                                    '/yr' + str(init_dates[0].year) + \n",
    "                                    '/mn' + str(init_dates[0].month) + \n",
    "                                    '/OUTPUT.' + str(ensemble) + \n",
    "                                    '/' + fcst_filename, autoclose=True)\n",
    "        # Concatenate along 'ensemble' dimension/coordinate -----\n",
    "        ds_temp.coords['ensemble'] = ensemble\n",
    "        ds_fcst = xr.concat([ds_fcst, ds_temp],'ensemble')\n",
    "\n",
    "    # Resample to desired frequency and resave time as lead time -----\n",
    "    ds_fcst = ds_fcst.resample(freq=resample_freq, dim='time', how=resample_method) \\\n",
    "                               .isel(time = range(len(lead_times)))\n",
    "    ds_fcst['time'] = ds_fcst['time'].values.astype('<M8[' + resample_freq + ']')\n",
    "    ds_fcst = utils.datetime_to_leadtime(ds_fcst).expand_dims('init_date')\n",
    "    \n",
    "# ==============================================\n",
    "# Loop over remaining initialization dates -----\n",
    "# ==============================================\n",
    "for init_date in init_dates[1:]:\n",
    "    with utils.timer():\n",
    "        year = init_date.year\n",
    "        month = init_date.month\n",
    "        print(f'Gathering data for forecast started on {month}-{year}...')\n",
    "\n",
    "        # There is a bug in xarray that causes an 'invalid type promotion' sometimes when concatenating \n",
    "        # The following while loop provides a work-around \n",
    "        more_ensembles = True\n",
    "        first_chunk = True\n",
    "        current_ensemble = 1\n",
    "\n",
    "        while more_ensembles:\n",
    "            try:\n",
    "                # Initialize xarray object for first ensemble -----\n",
    "                ds_temp1 = xr.open_mfdataset(fcst_folder + \n",
    "                                             '/yr' + str(year) + \n",
    "                                             '/mn' + str(month) + \n",
    "                                             '/OUTPUT.' + str(ensembles[current_ensemble-1]) + \n",
    "                                             '/' + fcst_filename, autoclose=True)\n",
    "                ds_temp1.coords['ensemble'] = ensembles[current_ensemble-1]\n",
    "\n",
    "                for ensemble in ensembles[current_ensemble:]:\n",
    "                    ds_temp2 = xr.open_mfdataset(fcst_folder + \n",
    "                                                '/yr' + str(year) + \n",
    "                                                '/mn' + str(month) + \n",
    "                                                '/OUTPUT.' + str(ensemble) + \n",
    "                                                '/' + fcst_filename, autoclose=True)\n",
    "                    # Concatenate along 'ensemble' dimension/coordinate -----\n",
    "                    ds_temp2.coords['ensemble'] = ensemble\n",
    "                    ds_temp1 = xr.concat([ds_temp1, ds_temp2],'ensemble')\n",
    "\n",
    "                # try:\n",
    "                if first_chunk:\n",
    "                    ds_chunk = ds_temp1\n",
    "                else:\n",
    "                    ds_chunk = xr.concat([ds_chunk, ds_temp1],'ensemble')\n",
    "                # except:\n",
    "                #     type, value, tb = sys.exc_info()\n",
    "                #     traceback.print_exc()\n",
    "                #     pdb.post_mortem(tb)\n",
    "\n",
    "                more_ensembles = False\n",
    "            except TypeError:\n",
    "                if first_chunk:\n",
    "                    ds_chunk = ds_temp1\n",
    "                    first_chunk = False\n",
    "                else:\n",
    "                    ds_chunk = xr.concat([ds_chunk, ds_temp1],'ensemble')\n",
    "                current_ensemble = ensemble\n",
    "\n",
    "        # Resample to desired frequency and resave time as lead time -----\n",
    "        ds_chunk = ds_chunk.resample(freq=resample_freq, dim='time', how=resample_method) \\\n",
    "                           .isel(time = range(len(lead_times)))\n",
    "        ds_chunk['time'] = ds_chunk['time'].values.astype('<M8[' + resample_freq + ']')\n",
    "        ds_chunk = utils.datetime_to_leadtime(ds_chunk).expand_dims('init_date')\n",
    "        \n",
    "        # Concaneate along 'init_date' dimension/coordinate -----\n",
    "        ds_fcst = xr.concat([ds_fcst, ds_chunk],'init_date')\n",
    "\n",
    "# There seems to be a bug that re-adds the 'time' dimension after renaming - drop this -----\n",
    "ds_fcst = utils.prune(ds_fcst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "    ds_fcst = ds_fcst.chunk(chunks={'ensemble' : len(ensembles), 'lead_time' : len(lead_times)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct observations xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_years = [pd.to_datetime(init_dates).year]\n",
    "fcst_year_min = min(fcst_years)[0]\n",
    "fcst_year_max = max(fcst_years)[-1] + FCST_LENGTH\n",
    "\n",
    "# ===================================================\n",
    "# Only load years for which forecast data exist -----\n",
    "# ===================================================\n",
    "with utils.timer():\n",
    "    print(f'Gathering data for observations...')\n",
    "    \n",
    "    ds_jra = xr.open_mfdataset(obsv_folder + obsv_filename + str(fcst_year_min) + '*', \n",
    "                                 autoclose=True)\n",
    "    for year_to_load in range(fcst_year_min+1,fcst_year_max+1):\n",
    "        ds_temp2 = xr.open_mfdataset(obsv_folder + obsv_filename + str(year_to_load) + '*', \n",
    "                                     autoclose=True)\n",
    "        ds_jra = xr.concat([ds_jra, ds_temp2],'initial_time0_hours')\n",
    "\n",
    "    # Standardize naming -----\n",
    "    ds_jra = ds_jra.rename({'initial_time0_hours':'time',\n",
    "                                      'g0_lon_3':'lon',\n",
    "                                      'g0_lat_2':'lat',\n",
    "                                      'TPRAT_GDS0_SFC_ave3h':'precip'})\n",
    "\n",
    "    # Resample to desired frequency -----\n",
    "    ds_jra = ds_jra.resample(freq=resample_freq, dim='time', how=resample_method)\n",
    "    ds_jra['time'] = ds_jra['time'].values.astype('<M8[' + resample_freq + ']')\n",
    "    \n",
    "    # ===============================================\n",
    "    # Stack to resemble ds_forecast coordinates -----\n",
    "    # ===============================================\n",
    "    ds_obsv = utils.stack_by_init_date(ds_jra,init_dates,24)\n",
    "    \n",
    "ds_obsv = utils.prune(ds_obsv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "    ds_obsv = ds_obsv.chunk(chunks={'init_date' : len(init_dates)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at average monthly rainfall over Tasmania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract forecast and observation over region\n",
    "Note we `compute()` the xarray objects here to save time later on. Once dask is working, it will probably be most sensible to leave the objects uncomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region of interest -----\n",
    "region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "da_fcst = utils.calc_boxavg_latlon(ds_fcst['precip'] * 60 * 60 * 24 / 998.2 * 1000, region).compute()\n",
    "\n",
    "# The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "da_obsv = utils.calc_boxavg_latlon(1 / 8 * ds_obsv['precip'].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load climatology data\n",
    "Various climatologies are/will be accessable using utils.load_climatology(). Here we use a climatology computed over the full 55 year jra reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jra_clim = utils.load_climatology('jra_1958-2016', 'precip', freq='M')\n",
    "\n",
    "da_clim = utils.calc_boxavg_latlon(jra_clim, region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute anomaly data\n",
    "Recall that the forecast and observation data are saved as functions of lead time and initial date. The function `utils.anomalize()` computes anomalies given data and a climatology which each have a datetime dimension `time`. Thus it is necessary to first convert from the lead time/initial date format to a datetime format, then compute the anomaly, the convert back to the lead time/initial date format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalize = lambda data, clim: utils.datetime_to_leadtime(\n",
    "                                   utils.anomalize(\n",
    "                                       utils.leadtime_to_datetime(data),clim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_fcst_anom = da_fcst.groupby('init_date').apply(anomalize, clim=da_clim)\n",
    "\n",
    "da_obsv_anom = da_obsv.groupby('init_date').apply(anomalize, clim=da_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute persistence data\n",
    "This requires repeating the data at the first lead time over all lead times. `utils.repeat_data()` allows us to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pers = utils.repeat_data(da_obsv,'lead_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before computing any metrics, lets make some example plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the forecast ensembles and observations for the first initial date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(da_fcst['lead_time'],da_fcst.isel(init_date=[0]).squeeze())\n",
    "ax.plot(da_obsv['lead_time'],da_obsv.isel(init_date=[0]).squeeze(),'k-',linewidth=2)\n",
    "ax.set_xlabel('lead time')\n",
    "ax.set_ylabel('monthly rainfall [mm]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the forecast and observation anomalies for the first initial date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(da_fcst['lead_time'],da_fcst_anom.isel(init_date=[0]).squeeze())\n",
    "ax.plot(da_obsv['lead_time'],da_obsv_anom.isel(init_date=[0]).squeeze(),'k-',linewidth=2)\n",
    "ax.set_xlabel('lead time')\n",
    "ax.set_ylabel('monthly rainfall [mm]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "The climatology and persistence data are not actually used here, since this notebook demonstrates skill metrics for probabilistic and event-based forecasts. Currently we only have access to the mean climatology - i.e. we cannot determine climatological probabilities of events occurring. I plan to instead load saved fields of the climatological PDFs which will enable climatological probabilities to be computed for any user-specified event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for probabilistic forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for the event of monthly rainfall over Tasmania being greater than 100 mm/month but less than 600 mm/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = '(> 100) and (< 600)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute reliability as a function of lead time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the reliability -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    reliability = skill.compute_reliability(fcst_likelihood,obsv_logical,\n",
    "                                            fcst_probabilities,indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(lead_times)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        ax.plot([0, 1],[0, 1],'k--')\n",
    "        sample_clim = reliability['relative_freq'].isel(lead_time=idx, drop=True).mean()\n",
    "        ax.plot([-1, 2],[sample_clim, sample_clim],'k--')\n",
    "        ax.plot(reliability['forecast_probability'],\n",
    "                reliability['relative_freq'].isel(lead_time=idx, drop=True),'r',linewidth=2)\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Relative frequency')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('Forecast probability')\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        box = ax.get_position()\n",
    "        width = box.width\n",
    "        height = box.height\n",
    "        subpos = [0.05,0.65,0.3,0.3]\n",
    "        inax_position  = ax.transAxes.transform(subpos[0:2])\n",
    "        transFigure = fig.transFigure.inverted()\n",
    "        infig_position = transFigure.transform(inax_position)    \n",
    "        x = infig_position[0]\n",
    "        y = infig_position[1]\n",
    "        width *= subpos[2]\n",
    "        height *= subpos[3] \n",
    "        subax = fig.add_axes([x,y,width,height])\n",
    "        subax.yaxis.tick_right()\n",
    "        subax.bar(reliability['forecast_probability'],reliability['fcst_number'].isel(lead_time=idx, drop=True),\n",
    "                  width=reliability['forecast_probability'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute reliability across all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the reliability -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    reliability = skill.compute_reliability(fcst_likelihood,obsv_logical,fcst_probabilities,\n",
    "                                            indep_dims=['init_date','lead_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,6))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot([0, 1],[0, 1],'k--')\n",
    "    sample_clim = reliability['relative_freq'].mean()\n",
    "    ax.plot([-1, 2],[sample_clim, sample_clim],'k--')\n",
    "    ax.plot(reliability['forecast_probability'],reliability['relative_freq'],'r',linewidth=2)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlabel('Forecast probability')\n",
    "    ax.set_ylabel('Relative frequency');\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    subpos = [0.05,0.65,0.3,0.3]\n",
    "    inax_position  = ax.transAxes.transform(subpos[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= subpos[2]\n",
    "    height *= subpos[3] \n",
    "    subax = fig.add_axes([x,y,width,height])\n",
    "    subax.yaxis.tick_right()\n",
    "    subax.bar(reliability['forecast_probability'],reliability['fcst_number'],\n",
    "              width=reliability['forecast_probability'][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Brier scores as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the Brier score -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])-6)\n",
    "    Brier = skill.compute_Brier_score(fcst_likelihood,obsv_logical,fcst_prob=fcst_probabilities,\n",
    "                                      indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_reliability'],linewidth=1)\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_resolution'],linewidth=1)\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_uncertainty'],linewidth=1)\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_total'],linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Brier score')\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative operating characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ROC diagrams as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the roc -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    roc = skill.compute_roc(fcst_likelihood, obsv_logical, fcst_probabilities, \n",
    "                            indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(lead_times)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        ax.plot([-1, 2],[-1, 2],'k--')\n",
    "        ax.plot(roc['false_alarm_rate'].isel(lead_time=idx, drop=True),\n",
    "                roc['hit_rate'].isel(lead_time=idx, drop=True),'ro-',linewidth=2)\n",
    "        ax.set_xlim(-0.02,1.02)\n",
    "        ax.set_ylim(-0.02,1.02)\n",
    "        ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Hit rate')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('False alarm rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ROC diagram for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the roc -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    roc = skill.compute_roc(fcst_likelihood, obsv_logical, fcst_probabilities, \n",
    "                            indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,6))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot([-1, 2],[-1, 2],'k--')\n",
    "    ax.plot(roc['false_alarm_rate'],roc['hit_rate'],'ro-',linewidth=2)\n",
    "    ax.set_xlim(-0.02,1.02)\n",
    "    ax.set_ylim(-0.02,1.02)\n",
    "    ax.set_xlabel('Hit rate')\n",
    "    ax.set_ylabel('False alarm rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute discrimination diagrams as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the discrimination -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    discrimination = skill.compute_discrimination(fcst_likelihood, obsv_logical, \n",
    "                                                   fcst_probabilities, indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(lead_times)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        scale_width = 2.5\n",
    "        ax.bar(discrimination.bins-discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_obsved'].isel(lead_time=idx, drop=True),\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='b')\n",
    "        ax.bar(discrimination.bins+discrimination.bins[1]/scale_width/2,\n",
    "                discrimination['hist_not_obsved'].isel(lead_time=idx, drop=True),\n",
    "                width=discrimination.bins[1]/scale_width,\n",
    "                color='r')\n",
    "        max_count = max([discrimination['hist_obsved'].isel(lead_time=idx, drop=True).max(), \n",
    "                         discrimination['hist_not_obsved'].isel(lead_time=idx, drop=True).max()])\n",
    "        ax.text(0.9,0.85*max_count,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Likelihood')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('Forecast probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute discrimination diagram for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    discrimination = skill.compute_discrimination(fcst_likelihood, obsv_logical, \n",
    "                                                  fcst_probabilities, indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,3))\n",
    "\n",
    "    ax1 = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax1.grid()\n",
    "    scale_width = 2.5\n",
    "    ax1.bar(discrimination.bins-discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_obsved'],\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='b')\n",
    "    ax1.bar(discrimination.bins+discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_not_obsved'],\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='r')\n",
    "    ax1.set_xlabel('Forecast probability')\n",
    "    ax1.set_ylabel('Likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for categorized forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for 4 categories between 25 and 150 mm of monthly rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category edges -----\n",
    "category_edges = np.linspace(25,150,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute contingency as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute contingency table -----\n",
    "    contingency = skill.compute_contingency_table(da_fcst,da_obsv, category_edges,\n",
    "                                                  ensemble_dim='ensemble', indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(lead_times)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(10,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        im = ax.imshow(contingency.isel(lead_time=idx, drop=True))\n",
    "        # ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Forecast category')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('Observed category')\n",
    "\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax);\n",
    "    cbar_ax.set_ylabel('counts', rotation=270, labelpad=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    accuracy_score = skill.compute_accuracy_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heidke skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    Heidke_score = skill.compute_Heidke_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peirce skill score / Hanssen and Kuipers discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    Peirce_score = skill.compute_Peirce_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerrity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    Gerrity_score = skill.compute_Gerrity_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(accuracy_score['lead_time'],accuracy_score,linewidth=2)\n",
    "    ax.plot(Heidke_score['lead_time'],Heidke_score,linewidth=2)\n",
    "    ax.plot(Peirce_score['lead_time'],Peirce_score,linewidth=2)\n",
    "    ax.plot(Gerrity_score['lead_time'],Gerrity_score,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Score');\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for dichotomously categorized forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for monthly rainfall being > or < 100 mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Define category edges -----\n",
    "    category_edges = [-np.inf, 100, np.inf]\n",
    "\n",
    "    # Compute contingency table -----\n",
    "    contingency = skill.compute_contingency_table(da_fcst,da_obsv,category_edges,\n",
    "                                                  ensemble_dim='ensemble',indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    bias_score = skill.compute_bias_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    hit_rate = skill.compute_hit_rate(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    false_alarm_ratio = skill.compute_false_alarm_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    false_alarm_rate = skill.compute_false_alarm_rate(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    success_ratio = skill.compute_success_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    threat_score = skill.compute_threat_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equitable threat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    equit_threat_score = skill.compute_equit_threat_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    odds_ratio = skill.compute_odds_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    odds_ratio_skill = skill.compute_odds_ratio_skill(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(bias_score['lead_time'],bias_score,linewidth=2)\n",
    "    ax.plot(hit_rate['lead_time'],hit_rate,linewidth=2)\n",
    "    ax.plot(false_alarm_ratio['lead_time'],false_alarm_ratio,linewidth=2)\n",
    "    ax.plot(false_alarm_rate['lead_time'],false_alarm_rate,linewidth=2)\n",
    "    ax.plot(success_ratio['lead_time'],success_ratio,linewidth=2)\n",
    "    ax.plot(threat_score['lead_time'],threat_score,linewidth=2)\n",
    "    ax.plot(equit_threat_score['lead_time'],equit_threat_score,linewidth=2)\n",
    "    # ax.plot(odds_ratio['lead_time'],odds_ratio,linewidth=2)\n",
    "    ax.plot(odds_ratio_skill['lead_time'],odds_ratio_skill,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Score');\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with utils.timer():\n",
    "#     client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dts3_env]",
   "language": "python",
   "name": "conda-env-dts3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
