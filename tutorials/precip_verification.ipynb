{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial demonstrating verification of v1 precip against jra55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pyLatte package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatte import utils\n",
    "from pylatte import skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently, the following packages are required to load the data - this process will be replaced by the CAFE cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some plotting packages and widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import warnings    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Jupyter specific -----\n",
    "from ipywidgets import FloatProgress\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note about the methodology of pyLatte\n",
    "The pyLatte package is constructed around the xarray Python package. This is particularly useful for verification computations which require large numbers of samples (different model runs) to converge. \n",
    "\n",
    "The approach here is to generate very large xarray objects that reference all data required for the verification, but do not store the data in memory. Operations are performed on these xarray objects out-of-memory. When it is necessary to perform a compute (e.g. to produce a plot), this is distributed over multiple processors using the dask Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise dask (currently not working on vm31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import distributed\n",
    "# client = distributed.Client(local_dir='/tmp/squ027-dask-worker-space', n_workers=4)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct xarray objects for forecasts and observations\n",
    "(The CAFE cookbook will replace these code blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of forecast data -----\n",
    "fcst_folder = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/'\n",
    "fcst_filename = 'atmos_daily*'\n",
    "fcst_variable = 'precip'\n",
    "\n",
    "# Location of observation data -----\n",
    "obsv_folder = '/OSM/CBR/OA_DCFP/data/observations/jra55/isobaric/061_tprat/cat/'\n",
    "obsv_filename = 'jra.55.tprat.000.1958010100_2016123121.nc'\n",
    "obsv_variable = 'TPRAT_GDS0_SFC_ave3h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dates to include (takes approximately 1 min 30 sec per date) -----\n",
    "init_dates = pd.date_range('2003-1','2012-12' , freq='1MS')\n",
    "\n",
    "# Ensembles to include -----\n",
    "ensembles = range(1,12)\n",
    "\n",
    "# Forecast length -----\n",
    "FCST_LENGTH = 2 # years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling details -----\n",
    "resample_freq = 'MS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct forecasts xarray object\n",
    "Note, dask has a known bug that manifests when trying to concatentate data containing timedelta64 arrays (see https://github.com/pydata/xarray/issues/1952 for further details). For example, try to concatenate the following two Datasets:\n",
    "\n",
    "`In : path = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/yr2002/mn7/'`\n",
    "\n",
    "`In : ens5 = xr.open_mfdataset(path + 'OUTPUT.5/atmos_daily*.nc', autoclose=True)`\n",
    "\n",
    "`In : ens6 = xr.open_mfdataset(path + 'OUTPUT.6/atmos_daily*.nc', autoclose=True)`\n",
    "\n",
    "`In : xr.concat([ens5, ens6],'ensemble')`\n",
    "\n",
    "`Out : TypeError: invalid type promotion`\n",
    "\n",
    "The error here is actually caused by the variables `average_DT` and `time_bounds`, which are timedelta64 arrays. However, I still do not fully unstand the bug: concatenation of `ens4` and `ens5`, for example, works fine, even though `ens4` also contains the timedelta64 variables `average_DT` and `time_bounds`. Regardless, because of this bug, it is not possible currently to create an xarray Dataset object containing all model variables. Instead, only the variable of interest (i.e. `fcst_variable` and `obsv_variable`) are retained in the concatenated xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00623d332c9b46fc9952d2483df15fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0, description='Loading...', max=1320.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate progress bar -----\n",
    "f = FloatProgress(min=0, max=len(init_dates)*len(ensembles), description='Loading...') \n",
    "display(f)\n",
    "\n",
    "# Loop over initial dates -----\n",
    "fcst_list = []\n",
    "for init_date in init_dates:\n",
    "    year = init_date.year\n",
    "    month = init_date.month\n",
    "    \n",
    "    # Loop over ensembles -----\n",
    "    ens_list = []\n",
    "    for ensemble in ensembles:\n",
    "        # Signal to increment the progress bar -----\n",
    "        f.value += 1 \n",
    "        \n",
    "        # Stack ensembles into a list -----\n",
    "        path = fcst_folder + '/yr' + str(year) + '/mn' + str(month) + \\\n",
    "               '/OUTPUT.' + str(ensemble) + '/' + fcst_filename + '.nc'\n",
    "        dataset = xr.open_mfdataset(path, autoclose=True)[fcst_variable]\n",
    "        ens_list.append(dataset.resample(time=resample_freq) \\\n",
    "                               .sum(dim='time'))\n",
    "        \n",
    "    # Concatenate ensembles -----\n",
    "    ens_object = xr.concat(ens_list, dim='ensemble')\n",
    "    ens_object['ensemble'] = ensembles\n",
    "    \n",
    "    # Stack concatenated ensembles into a list for each initial date -----                       \n",
    "    fcst_list.append(utils.datetime_to_leadtime(ens_object))\n",
    "\n",
    "# Keep track of the lead time for each initialization -----\n",
    "n_lead_time = [len(x.lead_time) for x in fcst_list]\n",
    "\n",
    "# Concatenate initial dates -----\n",
    "da_fcst = xr.concat(fcst_list, dim='init_date')\n",
    "\n",
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "da_fcst = utils.prune(da_fcst.chunk(chunks={'ensemble' : len(da_fcst.ensemble), \n",
    "                                            'lead_time' : len(da_fcst.lead_time)}).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate the forecast lead times at 2 years\n",
    "The January and July forecasts are run for 5 years rather than 2 years. The xarray concatenation above can deal with this, but fills the shorter forecasts with nans for lead times longer than 2 years. Let's get rid of some of these nans by truncating the forecasts at the lead time corresponding to the longest 2 year forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_increments = FCST_LENGTH * 12\n",
    "n_trunc = max([i for i in n_lead_time if i <= max_increments])\n",
    "da_fcst = da_fcst.isel(lead_time=range(n_trunc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct observations xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate progress bar -----\n",
    "f = FloatProgress(min=0, max=1, description='Loading...') \n",
    "display(f)\n",
    "\n",
    "# JRA temperature fields are only save in a time-concatenated form -----\n",
    "path = obsv_folder + obsv_filename\n",
    "dataset = xr.open_mfdataset(path, autoclose=True)[obsv_variable]\n",
    "da_obsv = dataset.rename(fcst_variable) \\\n",
    "                 .rename({'initial_time0_hours' : 'time', 'g0_lon_3' : 'lon', 'g0_lat_2' : 'lat'}) \\\n",
    "                 .resample(time=resample_freq) \\\n",
    "                 .sum(dim='time')\n",
    "\n",
    "# Stack by initial date to match forecast structure -----\n",
    "da_obsv = utils.stack_by_init_date(da_obsv,da_fcst.init_date.values,n_trunc)\n",
    "f.value += 1\n",
    "\n",
    "# Average over forecast dimension if it is exists -----\n",
    "if 'forecast_time1' in da_obsv.coords:\n",
    "    da_obsv = da_obsv.mean(dim='forecast_time1')\n",
    "\n",
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "da_obsv = utils.prune(da_obsv.chunk(chunks={'init_date' : len(da_obsv.init_date)}).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at average monthly rainfall over Tasmania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract forecast and observation over region\n",
    "Note, we `compute()` the xarray objects here to save time later on. Once dask is working, it will probably be most sensible to leave the objects uncomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Region of interest -----\n",
    "    region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "    unit_conv = 60 * 60 * 24 / 998.2 * 1000\n",
    "    da_fcst = utils.calc_boxavg_latlon(da_fcst * unit_conv, region).compute()\n",
    "\n",
    "    da_obsv = utils.calc_boxavg_latlon(da_obsv, region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load climatology data\n",
    "Various climatologies are/will be accessable using `utils.load_climatology()`. Here we use a climatology computed over the full 55 year jra reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jra_clim = utils.load_climatology('jra_1958-2016', 'precip', freq='MS')\n",
    "\n",
    "da_jra_clim = utils.calc_boxavg_latlon(jra_clim, region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute anomaly data\n",
    "Recall that the forecast and observation data are saved as functions of lead time and initial date. The function `utils.anomalize()` computes anomalies given data and a climatology which each have a datetime dimension `time`. Thus it is necessary to first convert from the lead time/initial date format to a datetime format, then compute the anomaly, the convert back to the lead time/initial date format. The functions `utils.datetime_to_leadtime()` and `utils.leadtime_to_datetime()` enable these types of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalize = lambda data, clim: utils.datetime_to_leadtime(\n",
    "                                   utils.anomalize(\n",
    "                                       utils.leadtime_to_datetime(data),clim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_fcst_anom = da_fcst.groupby('init_date').apply(anomalize, clim=da_jra_clim)\n",
    "\n",
    "da_obsv_anom = da_obsv.groupby('init_date').apply(anomalize, clim=da_jra_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute persistence data\n",
    "This requires repeating the data at the first lead time over all lead times. `utils.repeat_data()` allows us to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pers = utils.repeat_data(da_obsv,'lead_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute climatogoloy in lead time/inital date format\n",
    "This is really just for convenience below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_clim = (0 * da_obsv).groupby('init_date').apply(anomalize, clim=-da_jra_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before computing any metrics, lets make some example plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the forecast ensembles and observations for the first initial date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(da_fcst['lead_time'],da_fcst.isel(init_date=[0]).squeeze().transpose())\n",
    "ax.plot(da_obsv['lead_time'],da_obsv.isel(init_date=[0]).squeeze(),'k-',linewidth=2)\n",
    "ax.set_xlabel('lead time')\n",
    "ax.set_ylabel('monthly rainfall [mm]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the forecast and observation anomalies for the first initial date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(da_fcst['lead_time'],da_fcst_anom.isel(init_date=[0]).squeeze().transpose())\n",
    "ax.plot(da_obsv['lead_time'],da_obsv_anom.isel(init_date=[0]).squeeze(),'k-',linewidth=2)\n",
    "ax.set_xlabel('lead time')\n",
    "ax.set_ylabel('monthly rainfall [mm]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "The climatology and persistence data are not actually used here, since this notebook demonstrates skill metrics for probabilistic and event-based forecasts. Currently we only have access to the mean climatology - i.e. we cannot determine climatological probabilities of events occurring. I plan to instead load saved fields of the climatological PDFs which will enable climatological probabilities to be computed for any user-specified event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for probabilistic forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.g. for the event of monthly rainfall over Tasmania being greater than 100 mm/month but less than 600 mm/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = '(> 100) and (< 600)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute reliability as a function of lead time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the reliability -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    reliability = skill.compute_reliability(fcst_likelihood, obsv_logical,\n",
    "                                            fcst_probabilities, over_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(da_fcst.lead_time)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        ax.plot([0, 1],[0, 1],'k--')\n",
    "        sample_clim = reliability['relative_freq'].isel(lead_time=idx, drop=True).mean()\n",
    "        ax.plot([-1, 2],[sample_clim, sample_clim],'k--')\n",
    "        ax.plot(reliability['comparison_probability'],\n",
    "                reliability['relative_freq'].isel(lead_time=idx, drop=True),'r',linewidth=2)\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Relative frequency')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('Forecast probability')\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        box = ax.get_position()\n",
    "        width = box.width\n",
    "        height = box.height\n",
    "        subpos = [0.05,0.65,0.3,0.3]\n",
    "        inax_position  = ax.transAxes.transform(subpos[0:2])\n",
    "        transFigure = fig.transFigure.inverted()\n",
    "        infig_position = transFigure.transform(inax_position)    \n",
    "        x = infig_position[0]\n",
    "        y = infig_position[1]\n",
    "        width *= subpos[2]\n",
    "        height *= subpos[3] \n",
    "        subax = fig.add_axes([x,y,width,height])\n",
    "        subax.yaxis.tick_right()\n",
    "        subax.bar(reliability['comparison_probability'],reliability['cmp_number'].isel(lead_time=idx, drop=True),\n",
    "                  width=reliability['comparison_probability'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute reliability across all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the reliability -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    reliability = skill.compute_reliability(fcst_likelihood,obsv_logical,fcst_probabilities,\n",
    "                                            over_dims=['init_date','lead_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,6))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot([0, 1],[0, 1],'k--')\n",
    "    sample_clim = reliability['relative_freq'].mean()\n",
    "    ax.plot([-1, 2],[sample_clim, sample_clim],'k--')\n",
    "    ax.plot(reliability['comparison_probability'],reliability['relative_freq'],'r',linewidth=2)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.set_xlabel('Forecast probability')\n",
    "    ax.set_ylabel('Relative frequency');\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    subpos = [0.05,0.65,0.3,0.3]\n",
    "    inax_position  = ax.transAxes.transform(subpos[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= subpos[2]\n",
    "    height *= subpos[3] \n",
    "    subax = fig.add_axes([x,y,width,height])\n",
    "    subax.yaxis.tick_right()\n",
    "    subax.bar(reliability['comparison_probability'],reliability['cmp_number'],\n",
    "              width=reliability['comparison_probability'][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Brier scores as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the Brier score -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])-6)\n",
    "    Brier = skill.compute_Brier_score(fcst_likelihood, obsv_logical, cmp_prob=fcst_probabilities,\n",
    "                                      over_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_reliability'],linewidth=1)\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_resolution'],linewidth=1)\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_uncertainty'],linewidth=1)\n",
    "    ax.plot(Brier['lead_time'],Brier['Brier_total'],linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Brier score')\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative operating characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROC diagrams as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the roc -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    roc = skill.compute_roc(fcst_likelihood, obsv_logical, fcst_probabilities, \n",
    "                            over_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(da_fcst.lead_time)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        ax.plot([-1, 2],[-1, 2],'k--')\n",
    "        ax.plot(roc['false_alarm_rate'].isel(lead_time=idx, drop=True),\n",
    "                roc['hit_rate'].isel(lead_time=idx, drop=True),'ro-',linewidth=2)\n",
    "        ax.set_xlim(-0.02,1.02)\n",
    "        ax.set_ylim(-0.02,1.02)\n",
    "        ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Hit rate')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('False alarm rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ROC diagram for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the roc -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    roc = skill.compute_roc(fcst_likelihood, obsv_logical, fcst_probabilities, \n",
    "                            over_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,6))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot([-1, 2],[-1, 2],'k--')\n",
    "    ax.plot(roc['false_alarm_rate'],roc['hit_rate'],'ro-',linewidth=2)\n",
    "    ax.set_xlim(-0.02,1.02)\n",
    "    ax.set_ylim(-0.02,1.02)\n",
    "    ax.set_xlabel('Hit rate')\n",
    "    ax.set_ylabel('False alarm rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute discrimination diagrams as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Compute the event data for forecast likelihood and observations -----\n",
    "    fcst_likelihood = skill.compute_likelihood(skill.did_event(da_fcst, event))\n",
    "    obsv_logical = skill.did_event(da_obsv, event)\n",
    "\n",
    "    # Compute the discrimination -----\n",
    "    fcst_probabilities = np.linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "    discrimination = skill.compute_discrimination(fcst_likelihood, obsv_logical, \n",
    "                                                   fcst_probabilities, over_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(da_fcst.lead_time)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        scale_width = 2.5\n",
    "        ax.bar(discrimination.bins-discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_event'].isel(lead_time=idx, drop=True),\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='b')\n",
    "        ax.bar(discrimination.bins+discrimination.bins[1]/scale_width/2,\n",
    "                discrimination['hist_no_event'].isel(lead_time=idx, drop=True),\n",
    "                width=discrimination.bins[1]/scale_width,\n",
    "                color='r')\n",
    "        max_count = max([discrimination['hist_event'].isel(lead_time=idx, drop=True).max(), \n",
    "                         discrimination['hist_no_event'].isel(lead_time=idx, drop=True).max()])\n",
    "        ax.text(0.9,0.85*max_count,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Likelihood')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('Forecast probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute discrimination diagram for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    discrimination = skill.compute_discrimination(fcst_likelihood, obsv_logical, \n",
    "                                                  fcst_probabilities, over_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,3))\n",
    "\n",
    "    ax1 = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax1.grid()\n",
    "    scale_width = 2.5\n",
    "    ax1.bar(discrimination.bins-discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_event'],\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='b')\n",
    "    ax1.bar(discrimination.bins+discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_no_event'],\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='r')\n",
    "    ax1.set_xlabel('Forecast probability')\n",
    "    ax1.set_ylabel('Likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for categorized forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.g. for 4 categories between 25 and 150 mm of monthly rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_edges = np.linspace(25,150,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute contingency as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    contingency = skill.compute_contingency_table(da_fcst,da_obsv, category_edges, over_dims=['init_date','ensemble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(da_fcst.lead_time)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(10,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        im = ax.imshow(contingency.isel(lead_time=idx, drop=True))\n",
    "        # ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('Forecast category')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('Observed category')\n",
    "\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax);\n",
    "    cbar_ax.set_ylabel('counts', rotation=270, labelpad=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    accuracy_score = skill.compute_accuracy_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heidke skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    Heidke_score = skill.compute_Heidke_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peirce skill score / Hanssen and Kuipers discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    Peirce_score = skill.compute_Peirce_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerrity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    Gerrity_score = skill.compute_Gerrity_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(accuracy_score['lead_time'],accuracy_score,linewidth=2)\n",
    "    ax.plot(Heidke_score['lead_time'],Heidke_score,linewidth=2)\n",
    "    ax.plot(Peirce_score['lead_time'],Peirce_score,linewidth=2)\n",
    "    ax.plot(Gerrity_score['lead_time'],Gerrity_score,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Score');\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for dichotomously categorized forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.g. for monthly rainfall being > or < 100 mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_edges = [-np.inf, 100, np.inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute dichotomous contingency as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    contingency = skill.compute_contingency_table(da_fcst,da_obsv,category_edges,over_dims=['init_date','ensemble'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    bias_score = skill.compute_bias_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    hit_rate = skill.compute_hit_rate(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    false_alarm_ratio = skill.compute_false_alarm_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    false_alarm_rate = skill.compute_false_alarm_rate(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    success_ratio = skill.compute_success_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    threat_score = skill.compute_threat_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equitable threat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    equit_threat_score = skill.compute_equit_threat_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    odds_ratio = skill.compute_odds_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    odds_ratio_skill = skill.compute_odds_ratio_skill(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(bias_score['lead_time'],bias_score,linewidth=2)\n",
    "    ax.plot(hit_rate['lead_time'],hit_rate,linewidth=2)\n",
    "    ax.plot(false_alarm_ratio['lead_time'],false_alarm_ratio,linewidth=2)\n",
    "    ax.plot(false_alarm_rate['lead_time'],false_alarm_rate,linewidth=2)\n",
    "    ax.plot(success_ratio['lead_time'],success_ratio,linewidth=2)\n",
    "    ax.plot(threat_score['lead_time'],threat_score,linewidth=2)\n",
    "    ax.plot(equit_threat_score['lead_time'],equit_threat_score,linewidth=2)\n",
    "    # ax.plot(odds_ratio['lead_time'],odds_ratio,linewidth=2)\n",
    "    ax.plot(odds_ratio_skill['lead_time'],odds_ratio_skill,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Score');\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with utils.timer():\n",
    "#     client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dts3_env]",
   "language": "python",
   "name": "conda-env-dts3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
