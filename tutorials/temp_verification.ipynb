{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial demonstrating verification of v1 1000hPa temp against jra55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import pyLatte package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatte import utils\n",
    "from pylatte import skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently, the following packages are required to load the data - this process will be replaced by the CAFE cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "# Jupyter specific -----\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pyLatte package is constructed around the xarray Python package. This is particularly useful for verification computations which require large numbers of samples (different model runs) to converge. \n",
    "\n",
    "#### The approach here is to generate very large xarray objects that reference all data required for the verification, but do not store the data in memory. Operations are performed on these xarray objects out-of-memory. When it is necessary to perform a compute (e.g. to produce a plot), this is distributed over multiple processors using the dask Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise dask (currently not working on vm31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import distributed\n",
    "# client = distributed.Client(local_dir='/tmp/squ027-dask-worker-space', n_workers=4)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct xarray objects for forecasts and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (The CAFE cookbook will replace these code blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling details -----\n",
    "resample_freq = '1MS' # Must be '1MS' for monthly\n",
    "resample_method = 'mean'\n",
    "\n",
    "# Location of forecast data -----\n",
    "fcst_folder = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/'\n",
    "fcst_filename = 'atmos_daily*'\n",
    "fcst_variable = 'temp'\n",
    "\n",
    "# Location of observation data -----\n",
    "obsv_folder = '/OSM/CBR/OA_DCFP/data/observations/jra55/isobaric/011_tmp/cat/'\n",
    "obsv_filename = 'jra.55.tmp.1000.1958010100_2016123118.nc'\n",
    "obsv_variable = 'TMP_GDS0_ISBL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization dates (takes approximately 1 min 30 sec per date) -----\n",
    "init_dates = pd.date_range('2/2002','7/2002' , freq='1MS')  # Must be '1MS' for monthly\n",
    "\n",
    "# Ensembles to include -----\n",
    "ensembles = range(1,12)\n",
    "\n",
    "# Forecast length -----\n",
    "FCST_LENGTH = 2 # years\n",
    "no_leap = 2001\n",
    "n_incr = len(pd.date_range('1/1/' + str(no_leap),\n",
    "                           '12/1/' + str(no_leap+FCST_LENGTH-1),\n",
    "                           freq=resample_freq)) # number of lead_time increments\n",
    "lead_times = range(1,n_incr+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct forecasts xarray object\n",
    "Note, dask has a known bug that manifests when trying to concatentate data containing timedelta64 arrays (see https://github.com/pydata/xarray/issues/1952 for further details). For example, try to concatenate the following two Datasets:\n",
    "\n",
    "`In : path = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/yr2002/mn7/'`\n",
    "\n",
    "`In : ens5 = xr.open_mfdataset(path + 'OUTPUT.5/atmos_daily*.nc', autoclose=True)`\n",
    "\n",
    "`In : ens6 = xr.open_mfdataset(path + 'OUTPUT.6/atmos_daily*.nc', autoclose=True)`\n",
    "\n",
    "`In : xr.concat([ens5, ens6],'ensemble')`\n",
    "\n",
    "`Out : TypeError: invalid type promotion`\n",
    "\n",
    "The error here is actually caused by the variables `average_DT` and `time_bounds`, which are timedelta64 arrays. However, I still do not fully unstand the bug: concatenation of `ens4` and `ens5`, for example, works fine, even though `ens4` also contains the timedelta64 variables `average_DT` and `time_bounds`. Regardless, because of this bug, it is not possible currently to create an xarray Dataset object containing all model variables. Instead, only the variable of interest (i.e. `fcst_variable` and `obsv_variable`) are retained in the concatenated xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d4c194353c48ceb85e6b61830c4dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0, description='Loading...', max=66.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate progress bar -----\n",
    "f = FloatProgress(min=0, max=len(init_dates)*len(ensembles), description='Loading...') \n",
    "display(f)\n",
    "\n",
    "# Loop over initial dates -----\n",
    "fcst_list = []\n",
    "for init_date in init_dates:\n",
    "    year = init_date.year\n",
    "    month = init_date.month\n",
    "    \n",
    "    # Loop over ensembles -----\n",
    "    ens_list = []\n",
    "    for ensemble in ensembles:\n",
    "        # Signal to increment the progress bar -----\n",
    "        f.value += 1 \n",
    "        \n",
    "        # Stack ensembles into a list -----\n",
    "        path = fcst_folder + '/yr' + str(year) + '/mn' + str(month) + \\\n",
    "               '/OUTPUT.' + str(ensemble) + '/' + fcst_filename + '.nc'\n",
    "        ens_list.append(xr.open_mfdataset(path, autoclose=True)[fcst_variable])\n",
    "        \n",
    "    # Concatenate ensembles -----\n",
    "    ens_object = xr.concat(ens_list, dim='ensemble')\n",
    "    ens_object['ensemble'] = ensembles\n",
    "    \n",
    "    # Stack concatenated ensembles into a list for each initial date -----\n",
    "    fcst_list.append(ens_object)\n",
    "\n",
    "# Keep track of the lead time for each initialization -----\n",
    "n_lead_time = [len(x.time) for x in fcst_list]\n",
    "\n",
    "# Concatenate initial dates -----\n",
    "da_fcst = xr.concat(fcst_list, dim='init_date').rename({'time' : 'lead_time'})\n",
    "da_fcst['init_date'] = init_dates\n",
    "freq = pd.infer_freq(da_fcst.lead_time.values)\n",
    "da_fcst['lead_time'] = range(len(da_fcst.lead_time))\n",
    "da_fcst['lead_time'].attrs['units'] = freq\n",
    "\n",
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "da_fcst = da_fcst.chunk(chunks={'ensemble' : len(da_fcst.ensemble), 'lead_time' : len(da_fcst.lead_time)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate the forecast lead times at 2 years\n",
    "The January and July forecasts are run for 5 years rather than 2 years. The xarray concatenation above can deal with this, but fills the shorter forecasts with nans for lead times longer than 2 years. Let's get rid of some of these nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trunc = max([i for i in n_lead_time if i < 2000])\n",
    "da_fcst = da_fcst.isel(lead_time=range(n_trunc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct observations xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_by_init_date(data_in, init_dates, N_lead_steps, init_date_name='init_date', lead_time_name='lead_time'):\n",
    "    \"\"\" \n",
    "    Splits provided data array into n chunks beginning at time=init_date[n] and spanning \n",
    "    N_lead_steps time increments\n",
    "    Input Dataset/DataArray must span full range of times required for this operation\n",
    "    \"\"\"\n",
    "\n",
    "    init_list = []\n",
    "    for init_date in init_dates:\n",
    "        print(data_in.time)\n",
    "        print(np.datetime64(init_date))\n",
    "        start_index = np.where(data_in.time == np.datetime64(init_date))[0].item()\n",
    "        init_list.append(\n",
    "                      datetime_to_leadtime(\n",
    "                          data_in.isel(time=range(start_index, start_index + N_lead_steps))))\n",
    "    \n",
    "    data_out = xr.concat(init_list, dim='ensemble')\n",
    "    \n",
    "    # # Initialize xarray object for first initialization date -----\n",
    "    # start_index = np.where(data_in.time == np.datetime64(init_dates[0]))[0].item()\n",
    "    # data_out = data_in.isel(time=range(start_index, start_index + N_lead_steps))\n",
    "    # data_out = datetime_to_leadtime(data_out).expand_dims(init_date_name)\n",
    "    \n",
    "    # # Loop over remaining initialization dates -----\n",
    "    # for init_date in init_dates[1:]:\n",
    "    #     start_index = np.where(data_in.time == np.datetime64(init_date))[0].item()\n",
    "    #     data_temp = data_in.isel(time=range(start_index, start_index + N_lead_steps))\n",
    "\n",
    "        # # Concatenate along initialization date dimension/coordinate -----\n",
    "        # data_temp = datetime_to_leadtime(data_temp)\n",
    "        # data_out = xr.concat([data_out, data_temp],init_date_name) \n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee89dd975ade40b9a43e08bc5be20452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0, description='Loading...', max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 21550)>\n",
      "array(['1958-01-01T09:00:00.000000000', '1958-01-02T09:00:00.000000000',\n",
      "       '1958-01-03T09:00:00.000000000', ..., '2016-12-29T09:00:00.000000000',\n",
      "       '2016-12-30T09:00:00.000000000', '2016-12-31T09:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1958-01-01T09:00:00 1958-01-02T09:00:00 ...\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      initial time\n",
      "    bounds:         initial_time0_hours_bnds\n",
      "    axis:           T\n",
      "2002-02-01T00:00:00.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-7fef8e6c60b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mda_obsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_by_init_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda_obsv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_trunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-bab742c290a3>\u001b[0m in \u001b[0;36mstack_by_init_date\u001b[0;34m(data_in, init_dates, N_lead_steps, init_date_name, lead_time_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         init_list.append(\n\u001b[1;32m     14\u001b[0m                       datetime_to_leadtime(\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "# Instantiate progress bar -----\n",
    "f = FloatProgress(min=0, max=1, description='Loading...') \n",
    "display(f)\n",
    "\n",
    "# JRA temperature fields are only save in a time-concatenated form -----\n",
    "path = obsv_folder + obsv_filename\n",
    "da_obsv = (xr.open_mfdataset(path, autoclose=True)[obsv_variable]).rename(fcst_variable) \\\n",
    "                                                               .rename({'initial_time0_hours' : 'time',\n",
    "                                                                        'g0_lon_3' : 'lon',\n",
    "                                                                        'g0_lat_2' : 'lat'})\n",
    "f.value += 1 \n",
    "\n",
    "# \n",
    "\n",
    "da_obsv = stack_by_init_date(da_obsv,init_dates,n_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/OSM/CBR/OA_DCFP/data/observations/jra55/isobaric/011_tmp/cat/jra.55.tmp.1000.1958010100_2016123118.nc2003*.nc\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fbf37c9043bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobsv_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobsv_filename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_to_load\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*.nc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0myear_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_mfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobsv_variable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Concatenate years -----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, lock, data_vars, coords, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no files to open'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "# Compute forecast years -----\n",
    "fcst_years = [pd.to_datetime(init_dates).year]\n",
    "fcst_year_min = min(fcst_years)[0]\n",
    "fcst_year_max = max(fcst_years)[-1] + FCST_LENGTH\n",
    "\n",
    "# Loop over all years in forecasts -----\n",
    "year_list = []\n",
    "for year_to_load in range(fcst_year_min+1,fcst_year_max+1):\n",
    "    path = obsv_folder + obsv_filename + str(year_to_load) + '*.nc'\n",
    "    print(path)\n",
    "    year_list.append(xr.open_mfdataset(path, autoclose=True)[obsv_variable])\n",
    "\n",
    "# Concatenate years -----\n",
    "ens_object = xr.concat(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d5e4043aedf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_obsv_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_mfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsv_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobsv_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobsv_variable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_dataarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_listed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             _, name, variable = _get_virtual_variable(\n\u001b[0;32m--> 783\u001b[0;31m                 self._variables, name, self._level_coords, self.dims)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/OSM/CBR/OA_DCFP/apps/squ027/anaconda3/envs/dts3_env/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_get_virtual_variable\u001b[0;34m(variables, key, level_vars, dim_sizes)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_index_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mref_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temp'"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "ds_obsv_all = xr.open_mfdataset(obsv_folder + obsv_filename, autoclose=True)[obsv_variable]\n",
    "\n",
    "ds_obsv_all = xr.open_mfdataset(obsv_folder + filename + str(fcst_year_min) + '*', \n",
    "                                 autoclose=True)\n",
    "    for year_to_load in range(fcst_year_min+1,fcst_year_max+1):\n",
    "        ds_temp2 = xr.open_mfdataset(obsv_folder + filename + str(year_to_load) + '*', \n",
    "                                     autoclose=True)\n",
    "        ds_jra = xr.concat([ds_jra, ds_temp2],'initial_time0_hours')\n",
    "\n",
    "    # Standardize naming -----\n",
    "    ds_jra = ds_jra.rename({'initial_time0_hours':'time',\n",
    "                                      'g0_lon_3':'lon',\n",
    "                                      'g0_lat_2':'lat',\n",
    "                                      'TPRAT_GDS0_SFC_ave3h':'precip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obsv_all = xr.open_mfdataset(obsv_folder + obsv_filename, autoclose=True)[obsv_variable]\n",
    "\n",
    "# Standardize naming -----\n",
    "ds_temp1 = ds_temp1.rename({'initial_time0_hours':'time',\n",
    "                                  'g0_lon_3':'lon',\n",
    "                                  'g0_lat_2':'lat',\n",
    "                                  obsv_variable : fcst_variable})\n",
    "\n",
    "# Resample to desired frequency -----\n",
    "ds_temp1 = ds_temp1.resample(freq=resample_freq, dim='time', how=resample_method)\n",
    "\n",
    "# ===============================================\n",
    "# Stack to resemble ds_forecast coordinates -----\n",
    "# ===============================================\n",
    "# Initialize xarray object for first lead_time -----\n",
    "start_index = np.where(ds_temp1.time == np.datetime64(init_dates[0]))[0].item()\n",
    "ds_obsv = ds_temp1.isel(time=range(start_index, start_index+len(lead_times)))\n",
    "ds_obsv.coords['init_date'] = init_dates[0]\n",
    "ds_obsv = ds_obsv.expand_dims('init_date')\n",
    "ds_obsv = ds_obsv.rename({'time' : 'lead_time'})\n",
    "ds_obsv['lead_time'] = lead_times\n",
    "\n",
    "# Loop over remaining lead_time -----\n",
    "for init_date in init_dates[1:]:\n",
    "    start_index = np.where(ds_temp1.time == np.datetime64(init_date))[0].item()\n",
    "    ds_temp3 = ds_temp1.isel(time=range(start_index, start_index+len(lead_times)))\n",
    "\n",
    "    # Concatenate along 'lead_time' dimension/coordinate -----\n",
    "    ds_temp3 = ds_temp3.rename({'time' : 'lead_time'})\n",
    "    ds_temp3['lead_time'] = lead_times\n",
    "    ds_temp3.coords['init_date'] = init_date\n",
    "    ds_obsv = xr.concat([ds_obsv, ds_temp3],'init_date') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "    ds_obsv = ds_obsv.chunk(chunks={'init_date' : len(init_dates)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for probabilistic forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for temperature at 1000hPa averaged over Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Region of interest -----\n",
    "    region = (-38.0, -11.0, 113.0 , 153.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "    da_fcst = utils.calc_boxavg_latlon(ds_fcst['temp']\n",
    "                                       .sel(pfull=1000,method='nearest',drop=True), region).compute()-273.15\n",
    "    da_obsv = utils.calc_boxavg_latlon(ds_obsv['temp']\n",
    "                                       .squeeze().drop('lv_ISBL1'), region).compute()-273.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot one initialization date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(10,5))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(da_fcst['lead_time'],da_fcst.isel(init_date=[0]).squeeze())\n",
    "ax.plot(da_obsv['lead_time'],da_obsv.isel(init_date=[0]).squeeze(),'k-',linewidth=2)\n",
    "ax.set_xlabel('lead time')\n",
    "ax.set_ylabel('average temp [K]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank the data and compute histograms as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    rank_histogram = skill.compute_rank_histogram(da_fcst, da_obsv, indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    ncol = 4; nrow = int(np.ceil(len(lead_times)/ncol));\n",
    "    fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "    for idx,ax in enumerate(axs.reshape(-1)): \n",
    "        ax.grid()\n",
    "        ax.bar(rank_histogram.bins,rank_histogram.isel(lead_time=idx, drop=True))\n",
    "        ax.set_ylim(0,rank_histogram.max())\n",
    "        ax.text(10.3,0.85*rank_histogram.max(),'mn '+str(idx+1))\n",
    "\n",
    "        if idx % ncol == 0:\n",
    "            ax.set_ylabel('count')\n",
    "\n",
    "        if idx / ncol >= nrow - 1:\n",
    "            ax.set_xlabel('bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank the data and compute histograms for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    rank_histogram = skill.compute_rank_histogram(da_fcst, da_obsv, indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,3))\n",
    "\n",
    "    ax1 = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax1.grid()\n",
    "    ax1.bar(rank_histogram.bins,rank_histogram)\n",
    "    ax1.set_xlabel('bins')\n",
    "    ax1.set_ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Continuous) ranked probability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    # Specify bins for computation of cdf -----\n",
    "    bins = np.linspace(0,40,100)\n",
    "\n",
    "    # Compute ranked probability score -----\n",
    "    rps = skill.compute_rps(da_fcst, da_obsv, bins=bins, indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(rps['lead_time'],rps,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Ranked probability score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill metrics for continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive bias error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_additive_bias = skill.compute_mean_additive_bias(da_fcst, da_obsv, \n",
    "                                                          indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicative bias error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_multiplicative_bias = skill.compute_mean_multiplicative_bias(da_fcst, da_obsv, \n",
    "                                                                      indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_absolute_error = skill.compute_mean_absolute_error(da_fcst, da_obsv, \n",
    "                                                            indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    mean_squared_error = skill.compute_mean_squared_error(da_fcst, da_obsv, \n",
    "                                                          indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    rms_error = skill.compute_rms_error(da_fcst, da_obsv, \n",
    "                                        indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "    ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax.grid()\n",
    "    ax.plot(mean_additive_bias['lead_time'],mean_additive_bias,linewidth=2)\n",
    "    ax.plot(mean_multiplicative_bias['lead_time'],mean_multiplicative_bias,linewidth=2)\n",
    "    ax.plot(mean_absolute_error['lead_time'],mean_absolute_error,linewidth=2)\n",
    "    # ax.plot(mean_squared_error['lead_time'],mean_squared_error,linewidth=2)\n",
    "    ax.plot(rms_error['lead_time'],rms_error,linewidth=2)\n",
    "    ax.set_xlabel('Lead time [months]')\n",
    "    ax.set_ylabel('Error');\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with utils.timer():\n",
    "#     client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dts3_env]",
   "language": "python",
   "name": "conda-env-dts3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
