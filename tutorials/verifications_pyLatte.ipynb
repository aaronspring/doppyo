{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial demonstrating use of verif module within pyLatte\n",
    "### To add pyLatte to python path:\n",
    " \n",
    "#### 1)   Clone the pyLatte Bitbucket repository to your local machine: \n",
    " \n",
    "`git clone https://<userid>@bitbucket.csiro.au/scm/df/pylatte.git`\n",
    " \n",
    "#### 2)   Within your .bashrc (or equivalent), add the pyLatte location to your PYTHONPATH:\n",
    "\n",
    "`PYTHONPATH=\"${PYTHONPATH}:/location/of/pyLatte/clone\"`\n",
    "\n",
    "`export PYTHONPATH`\n",
    " \n",
    "#### You should now be able to import the pyLatte package as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatte import utils\n",
    "from pylatte import verif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently, the following packages are required to load the data - this process will be replaced by the CAFE cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Jupyter specific -----\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pyLatte package is constructed around the xarray Python package. This is particularly useful for verification computations which require large numbers of samples (different model runs) to converge. \n",
    "\n",
    "#### The approach here is to generate very large xarray objects that reference all data required for the verification, but do not store the data in memory. Operations are performed on these xarray objects out-of-memory. When it is necessary to perform a compute (e.g. to produce a plot), this is distributed over multiple processors using the dask Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise dask (currently not working on vm31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import distributed\n",
    "# client = distributed.Client(local_dir='/tmp/squ027-dask-worker-space', n_workers=4)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct xarray objects for forecasts and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (The CAFE cookbook will replace these code blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling details -----\n",
    "resample_freq = '1MS' # Must be '1MS' for monthly\n",
    "resample_method = 'sum'\n",
    "\n",
    "# Location of forecast data -----\n",
    "fcst_folder = '/OSM/CBR/OA_DCFP/data/model_output/CAFE/forecasts/v1/'\n",
    "\n",
    "# Location of observation data -----\n",
    "obsv_folder = '/OSM/CBR/OA_DCFP/data/observations/jra55/isobaric/061_tprat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization dates (takes approximately 1 min 30 sec per date) -----\n",
    "init_dates = pd.date_range('2/2002','3/2002' , freq='1MS')  # Must be '1MS' for monthly\n",
    "\n",
    "# Ensembles to include -----\n",
    "ensembles = range(1,12)\n",
    "\n",
    "# Forecast length -----\n",
    "FCST_LENGTH = 2 # years\n",
    "no_leap = 2001\n",
    "n_incr = len(pd.date_range('1/1/' + str(no_leap),\n",
    "                           '12/1/' + str(no_leap+FCST_LENGTH-1),\n",
    "                           freq=resample_freq)) # number of lead_time increments\n",
    "lead_times = range(1,n_incr+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct forecasts xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb, traceback, sys\n",
    "filename = 'atmos_daily*'\n",
    "\n",
    "# ==================================================\n",
    "# Initialize xarray object for first init_date -----\n",
    "# ==================================================\n",
    "with utils.timer():\n",
    "    print(f'Gathering data for forecast started on {init_dates[0].month}-{init_dates[0].year}...')\n",
    "    \n",
    "    ds_fcst = xr.open_mfdataset(fcst_folder + \n",
    "                                '/yr' + str(init_dates[0].year) + \n",
    "                                '/mn' + str(init_dates[0].month) + \n",
    "                                '/OUTPUT.' + str(ensembles[0]) + \n",
    "                                '/' + filename, autoclose=True)\n",
    "    ds_fcst.coords['ensemble'] = ensembles[0]\n",
    "\n",
    "    for ensemble in ensembles[1:]:\n",
    "        ds_temp = xr.open_mfdataset(fcst_folder + \n",
    "                                    '/yr' + str(init_dates[0].year) + \n",
    "                                    '/mn' + str(init_dates[0].month) + \n",
    "                                    '/OUTPUT.' + str(ensemble) + \n",
    "                                    '/' + filename, autoclose=True)\n",
    "        # Concatenate along 'ensemble' dimension/coordinate -----\n",
    "        ds_temp.coords['ensemble'] = ensemble\n",
    "        ds_fcst = xr.concat([ds_fcst, ds_temp],'ensemble')\n",
    "\n",
    "    # Resample to desired frequency and resave time as lead time -----\n",
    "    ds_fcst = ds_fcst.resample(freq=resample_freq, dim='time', how=resample_method) \\\n",
    "                               .isel(time = range(len(lead_times)))\n",
    "    ds_fcst = ds_fcst.rename({'time' : 'lead_time'})\n",
    "    ds_fcst['lead_time'] = lead_times\n",
    "\n",
    "    # Initialize 'init_date' coordinate -----\n",
    "    ds_fcst.coords['init_date'] = init_dates[0]\n",
    "    ds_fcst = ds_fcst.expand_dims('init_date')\n",
    "    \n",
    "# ==============================================\n",
    "# Loop over remaining initialization dates -----\n",
    "# ==============================================\n",
    "for init_date in init_dates[1:]:\n",
    "    with utils.timer():\n",
    "        year = init_date.year\n",
    "        month = init_date.month\n",
    "        print(f'Gathering data for forecast started on {month}-{year}...')\n",
    "\n",
    "        # There is a bug in xarray that causes an 'invalid type promotion' sometimes when concatenating \n",
    "        # The following while loop provides a work-around \n",
    "        more_ensembles = True\n",
    "        first_chunk = True\n",
    "        current_ensemble = 1\n",
    "\n",
    "        while more_ensembles:\n",
    "            try:\n",
    "                # Initialize xarray object for first ensemble -----\n",
    "                ds_temp1 = xr.open_mfdataset(fcst_folder + \n",
    "                                             '/yr' + str(year) + \n",
    "                                             '/mn' + str(month) + \n",
    "                                             '/OUTPUT.' + str(ensembles[current_ensemble-1]) + \n",
    "                                             '/' + filename, autoclose=True)\n",
    "                ds_temp1.coords['ensemble'] = ensembles[current_ensemble-1]\n",
    "\n",
    "                for ensemble in ensembles[current_ensemble:]:\n",
    "                    ds_temp2 = xr.open_mfdataset(fcst_folder + \n",
    "                                                '/yr' + str(year) + \n",
    "                                                '/mn' + str(month) + \n",
    "                                                '/OUTPUT.' + str(ensemble) + \n",
    "                                                '/' + filename, autoclose=True)\n",
    "                    # Concatenate along 'ensemble' dimension/coordinate -----\n",
    "                    ds_temp2.coords['ensemble'] = ensemble\n",
    "                    ds_temp1 = xr.concat([ds_temp1, ds_temp2],'ensemble')\n",
    "\n",
    "                # try:\n",
    "                if first_chunk:\n",
    "                    ds_chunk = ds_temp1\n",
    "                else:\n",
    "                    ds_chunk = xr.concat([ds_chunk, ds_temp1],'ensemble')\n",
    "                # except:\n",
    "                #     type, value, tb = sys.exc_info()\n",
    "                #     traceback.print_exc()\n",
    "                #     pdb.post_mortem(tb)\n",
    "\n",
    "                more_ensembles = False\n",
    "            except TypeError:\n",
    "                if first_chunk:\n",
    "                    ds_chunk = ds_temp1\n",
    "                    first_chunk = False\n",
    "                else:\n",
    "                    ds_chunk = xr.concat([ds_chunk, ds_temp1],'ensemble')\n",
    "                current_ensemble = ensemble\n",
    "\n",
    "        # Resample to desired frequency and resave time as lead time -----\n",
    "        ds_chunk = ds_chunk.resample(freq=resample_freq, dim='time', how=resample_method) \\\n",
    "                           .isel(time = range(len(lead_times)))\n",
    "        ds_chunk = ds_chunk.rename({'time' : 'lead_time'})\n",
    "        ds_chunk['lead_time'] = lead_times\n",
    "\n",
    "        # Concaneate along 'init_date' dimension/coordinate -----\n",
    "        ds_chunk.coords['init_date'] = init_date\n",
    "        ds_fcst = xr.concat([ds_fcst, ds_chunk],'init_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "ds_fcst = ds_fcst.chunk(chunks={'ensemble' : len(ensembles), 'lead_time' : len(lead_times)})\n",
    "\n",
    "# Overwrite init_dates in case didn't fully load -----\n",
    "init_dates = ds_fcst.init_date.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct observations xarray object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fcst_phy2m125.061_tprat.'\n",
    "\n",
    "fcst_years = [pd.to_datetime(init_dates).year]\n",
    "fcst_year_min = min(fcst_years)[0]\n",
    "fcst_year_max = max(fcst_years)[-1] + FCST_LENGTH\n",
    "\n",
    "# ===================================================\n",
    "# Only load years for which forecast data exist -----\n",
    "# ===================================================\n",
    "with utils.timer():\n",
    "    print(f'Gathering data for observations...')\n",
    "    \n",
    "    ds_temp1 = xr.open_mfdataset(obsv_folder + filename + str(fcst_year_min) + '*')\n",
    "    for year_to_load in range(fcst_year_min+1,fcst_year_max+1):\n",
    "        ds_temp2 = xr.open_mfdataset(obsv_folder + filename + str(year_to_load) + '*')\n",
    "        ds_temp1 = xr.concat([ds_temp1, ds_temp2],'initial_time0_hours')\n",
    "\n",
    "    # Standardize naming -----\n",
    "    ds_temp1 = ds_temp1.rename({'initial_time0_hours':'time',\n",
    "                                      'g0_lon_3':'lon',\n",
    "                                      'g0_lat_2':'lat',\n",
    "                                      'TPRAT_GDS0_SFC_ave3h':'precip'})\n",
    "\n",
    "    # Resample to desired frequency -----\n",
    "    ds_temp1 = ds_temp1.resample(freq=resample_freq, dim='time', how=resample_method)\n",
    "    \n",
    "    # ===============================================\n",
    "    # Stack to resemble ds_forecast coordinates -----\n",
    "    # ===============================================\n",
    "    # Initialize xarray object for first lead_time -----\n",
    "    start_index = np.where(ds_temp1.time == np.datetime64(init_dates[0]))[0].item()\n",
    "    ds_obsv = ds_temp1.isel(time=range(start_index, start_index+len(lead_times)))\n",
    "    ds_obsv.coords['init_date'] = init_dates[0]\n",
    "    ds_obsv = ds_obsv.expand_dims('init_date')\n",
    "    ds_obsv = ds_obsv.rename({'time' : 'lead_time'})\n",
    "    ds_obsv['lead_time'] = lead_times\n",
    "    \n",
    "    # Loop over remaining lead_time -----\n",
    "    for init_date in init_dates[1:]:\n",
    "        start_index = np.where(ds_temp1.time == np.datetime64(init_date))[0].item()\n",
    "        ds_temp3 = ds_temp1.isel(time=range(start_index, start_index+len(lead_times)))\n",
    "\n",
    "        # Concatenate along 'lead_time' dimension/coordinate -----\n",
    "        ds_temp3 = ds_temp3.rename({'time' : 'lead_time'})\n",
    "        ds_temp3['lead_time'] = lead_times\n",
    "        ds_temp3.coords['init_date'] = init_date\n",
    "        ds_obsv = xr.concat([ds_obsv, ds_temp3],'init_date') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rechunk for chunksizes of at least 1,000,000 elements -----\n",
    "ds_obsv = ds_obsv.chunk(chunks={'init_date' : len(init_dates)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for probabilistic forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g., for precipitation averaged over region around Tasmania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.timer():\n",
    "    variable = 'precip'\n",
    "\n",
    "    # Region of interest -----\n",
    "    region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "    # Including unit conversion to mm/month\n",
    "    da_fcst = utils.calc_boxavg_latlon(ds_fcst[variable] * 60 * 60 * 24 / 998.2 * 1000, region).compute() \n",
    "\n",
    "    # The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "    da_obsv = utils.calc_boxavg_latlon(1 / 8 * ds_obsv[variable].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank the data and compute histograms as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rank_histogram = verif.compute_rank_histogram(da_fcst, da_obsv, indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 4; nrow = int(ceil(len(lead_times)/ncol));\n",
    "fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "for idx,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.grid()\n",
    "    ax.bar(rank_histogram.bins,rank_histogram.isel(lead_time=idx, drop=True))\n",
    "    ax.set_ylim(0,rank_histogram.max())\n",
    "    ax.text(10.3,0.85*rank_histogram.max(),'mn '+str(idx+1))\n",
    "    \n",
    "    if idx % ncol == 0:\n",
    "        ax.set_ylabel('count')\n",
    "        \n",
    "    if idx / ncol >= nrow - 1:\n",
    "        ax.set_xlabel('bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank the data and compute histograms for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_histogram = verif.compute_rank_histogram(da_fcst, da_obsv, indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,3))\n",
    "\n",
    "ax1 = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax1.grid()\n",
    "ax1.bar(rank_histogram.bins,rank_histogram)\n",
    "ax1.set_xlabel('bins')\n",
    "ax1.set_ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Continuous) ranked probability score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g., for precipitation averaged over region around Tasmania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'precip'\n",
    "\n",
    "# Region of interest -----\n",
    "region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "# Including unit conversion to mm/month\n",
    "# da_fcst = utils.calc_boxavg_latlon(ds_fcst[variable] * 60 * 60 * 24 / 998.2 * 1000, region).compute() \n",
    "\n",
    "# The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "# da_obsv = utils.calc_boxavg_latlon(1 / 8 * ds_obsv[variable].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute rps as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify bins for computation of cdf -----\n",
    "bins = linspace(0,200,20)\n",
    "\n",
    "# Compute ranked probability score -----\n",
    "rps = verif.compute_rps(da_fcst, da_obsv, bins=bins, indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(rps['lead_time'],rps,linewidth=2)\n",
    "ax.set_xlabel('Lead time [months]')\n",
    "ax.set_ylabel('Ranked probability score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for the event of monthly rainfall over Tasmania being greater than 60 mm/month but less than 200 mm/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'precip'\n",
    "\n",
    "event = '((> 60) and (< 200)) or (> 1000) or (= 500.0)' # for example\n",
    "\n",
    "# Region of interest -----\n",
    "# region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "# da_fcst = utils.calc_boxavg_latlon(ds_fcst[variable] * 60 * 60 * 24 / 998.2 * 1000, region).compute() \n",
    "\n",
    "# The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "# da_obsv = utils.calc_boxavg_latlon(1 / 8 * ds_obsv[variable].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute reliability as a function of lead time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the event data for forecast likelihood and observations -----\n",
    "fcst_likelihood = verif.compute_likelihood(verif.did_event(da_fcst, event))\n",
    "obsv_logical = verif.did_event(da_obsv, event)\n",
    "\n",
    "# Compute the reliability -----\n",
    "fcst_probabilities = linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "reliability = verif.compute_reliability(fcst_likelihood,obsv_logical,\n",
    "                                        fcst_probabilities,indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 4; nrow = int(ceil(len(lead_times)/ncol));\n",
    "fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "for idx,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.grid()\n",
    "    ax.plot([0, 1],[0, 1],'k--')\n",
    "    sample_clim = reliability['relative_freq'].isel(lead_time=idx, drop=True).mean()\n",
    "    ax.plot([-1, 2],[sample_clim, sample_clim],'k--')\n",
    "    ax.plot(reliability['forecast_probability'],\n",
    "            reliability['relative_freq'].isel(lead_time=idx, drop=True),'r',linewidth=2)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "    \n",
    "    if idx % ncol == 0:\n",
    "        ax.set_ylabel('Relative frequency')\n",
    "        \n",
    "    if idx / ncol >= nrow - 1:\n",
    "        ax.set_xlabel('Forecast probability')\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    subpos = [0.05,0.65,0.3,0.3]\n",
    "    inax_position  = ax.transAxes.transform(subpos[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= subpos[2]\n",
    "    height *= subpos[3] \n",
    "    subax = fig.add_axes([x,y,width,height])\n",
    "    subax.yaxis.tick_right()\n",
    "    subax.bar(reliability['forecast_probability'],reliability['fcst_number'].isel(lead_time=idx, drop=True),\n",
    "              width=reliability['forecast_probability'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute reliability across all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the event data for forecast likelihood and observations -----\n",
    "fcst_likelihood = verif.compute_likelihood(verif.did_event(da_fcst, event))\n",
    "obsv_logical = verif.did_event(da_obsv, event)\n",
    "\n",
    "# Compute the reliability -----\n",
    "fcst_probabilities = linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "reliability = verif.compute_reliability(fcst_likelihood,obsv_logical,fcst_probabilities,\n",
    "                                        indep_dims=['init_date','lead_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,6))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot([0, 1],[0, 1],'k--')\n",
    "sample_clim = reliability['relative_freq'].mean()\n",
    "ax.plot([-1, 2],[sample_clim, sample_clim],'k--')\n",
    "ax.plot(reliability['forecast_probability'],reliability['relative_freq'],'r',linewidth=2)\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_xlabel('Forecast probability')\n",
    "ax.set_ylabel('Relative frequency');\n",
    "\n",
    "fig = plt.gcf()\n",
    "box = ax.get_position()\n",
    "width = box.width\n",
    "height = box.height\n",
    "subpos = [0.05,0.65,0.3,0.3]\n",
    "inax_position  = ax.transAxes.transform(subpos[0:2])\n",
    "transFigure = fig.transFigure.inverted()\n",
    "infig_position = transFigure.transform(inax_position)    \n",
    "x = infig_position[0]\n",
    "y = infig_position[1]\n",
    "width *= subpos[2]\n",
    "height *= subpos[3] \n",
    "subax = fig.add_axes([x,y,width,height])\n",
    "subax.yaxis.tick_right()\n",
    "subax.bar(reliability['forecast_probability'],reliability['fcst_number'],\n",
    "          width=reliability['forecast_probability'][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for the event of monthly rainfall over Tasmania being greater than 100 mm/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'precip'\n",
    "\n",
    "event = '> 100' # for example\n",
    "\n",
    "# Region of interest -----\n",
    "# region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "# da_fcst = cafeutils.calc_boxavg_latlon(ds_fcst[variable] * 60 * 60 * 24 / 998.2 * 1000, region).compute() \n",
    "\n",
    "# The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "# da_obsv = cafeutils.calc_boxavg_latlon(1 / 8 * ds_obsv[variable].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Brier scores as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the event data for forecast likelihood and observations -----\n",
    "fcst_likelihood = verif.compute_likelihood(verif.did_event(da_fcst, event))\n",
    "obsv_logical = verif.did_event(da_obsv, event)\n",
    "\n",
    "# Compute the Brier score -----\n",
    "fcst_probabilities = linspace(0,1,len(da_fcst['ensemble'])-6)\n",
    "Brier = verif.compute_Brier_score(fcst_likelihood,obsv_logical,fcst_prob=fcst_probabilities,\n",
    "                                  indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(Brier['lead_time'],Brier['Brier_reliability'],linewidth=1)\n",
    "ax.plot(Brier['lead_time'],Brier['Brier_resolution'],linewidth=1)\n",
    "ax.plot(Brier['lead_time'],Brier['Brier_uncertainty'],linewidth=1)\n",
    "ax.plot(Brier['lead_time'],Brier['Brier_total'],linewidth=2)\n",
    "ax.set_xlabel('Lead time [months]')\n",
    "ax.set_ylabel('Brier score')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative operating characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for the event of monthly rainfall over Tasmania being greater than 100 mm/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'precip'\n",
    "\n",
    "event = '> 100' # for example\n",
    "\n",
    "# Region of interest -----\n",
    "# region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "# da_fcst = utils.calc_boxavg_latlon(ds_fcst[variable] * 60 * 60 * 24 / 998.2 * 1000, region).compute() \n",
    "\n",
    "# The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "# da_obsv = utils.calc_boxavg_latlon(1 / 8 * ds_obsv[variable].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ROC diagrams as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the event data for forecast likelihood and observations -----\n",
    "fcst_likelihood = verif.compute_likelihood(verif.did_event(da_fcst, event))\n",
    "obsv_logical = verif.did_event(da_obsv, event)\n",
    "\n",
    "# Compute the roc -----\n",
    "fcst_probabilities = linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "roc = verif.compute_roc(fcst_likelihood, obsv_logical, fcst_probabilities, \n",
    "                        indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 4; nrow = int(ceil(len(lead_times)/ncol));\n",
    "fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "for idx,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.grid()\n",
    "    ax.plot([-1, 2],[-1, 2],'k--')\n",
    "    ax.plot(roc['false_alarm_rate'].isel(lead_time=idx, drop=True),\n",
    "            roc['hit_rate'].isel(lead_time=idx, drop=True),'ro-',linewidth=2)\n",
    "    ax.set_xlim(-0.02,1.02)\n",
    "    ax.set_ylim(-0.02,1.02)\n",
    "    ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "    \n",
    "    if idx % ncol == 0:\n",
    "        ax.set_ylabel('Hit rate')\n",
    "        \n",
    "    if idx / ncol >= nrow - 1:\n",
    "        ax.set_xlabel('False alarm rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ROC diagram for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the event data for forecast likelihood and observations -----\n",
    "fcst_likelihood = verif.compute_likelihood(verif.did_event(da_fcst, event))\n",
    "obsv_logical = verif.did_event(da_obsv, event)\n",
    "\n",
    "# Compute the roc -----\n",
    "fcst_probabilities = linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "roc = verif.compute_roc(fcst_likelihood, obsv_logical, fcst_probabilities, \n",
    "                        indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,6))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot([-1, 2],[-1, 2],'k--')\n",
    "ax.plot(roc['false_alarm_rate'],roc['hit_rate'],'ro-',linewidth=2)\n",
    "ax.set_xlim(-0.02,1.02)\n",
    "ax.set_ylim(-0.02,1.02)\n",
    "ax.set_xlabel('Hit rate')\n",
    "ax.set_ylabel('False alarm rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for the event of monthly rainfall over Tasmania being greater than 100 mm/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'precip'\n",
    "\n",
    "event = '> 100' # for example\n",
    "\n",
    "# Region of interest -----\n",
    "# region = (-44.0, -40.0, 144.0 , 148.0) # (lat_min,lat_max,lon_min,lon_max)\n",
    "\n",
    "# da_fcst = utils.calc_boxavg_latlon(ds_fcst[variable] * 60 * 60 * 24 / 998.2 * 1000, region).compute() \n",
    "\n",
    "# The jra55 precip data is saved with 3hr and 6hr forecasts as an additional dimension - deal with these -----\n",
    "# da_obsv = utils.calc_boxavg_latlon(1 / 8 * ds_obsv[variable].sum(dim='forecast_time1'), region).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute discrimination diagrams as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the event data for forecast likelihood and observations -----\n",
    "fcst_likelihood = verif.compute_likelihood(verif.did_event(da_fcst, event))\n",
    "obsv_logical = verif.did_event(da_obsv, event)\n",
    "\n",
    "# Compute the discrimination -----\n",
    "fcst_probabilities = linspace(0,1,len(da_fcst['ensemble'])+1)\n",
    "discrimination = everif.compute_discrimination(fcst_likelihood, obsv_logical, \n",
    "                                               fcst_probabilities, indep_dims='init_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 4; nrow = int(ceil(len(lead_times)/ncol));\n",
    "fig, axs = plt.subplots(figsize=(15,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "for idx,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.grid()\n",
    "    scale_width = 2.5\n",
    "    ax.bar(discrimination.bins-discrimination.bins[1]/scale_width/2,\n",
    "        discrimination['hist_obsved'].isel(lead_time=idx, drop=True),\n",
    "        width=discrimination.bins[1]/scale_width,\n",
    "        color='b')\n",
    "    ax.bar(discrimination.bins+discrimination.bins[1]/scale_width/2,\n",
    "            discrimination['hist_not_obsved'].isel(lead_time=idx, drop=True),\n",
    "            width=discrimination.bins[1]/scale_width,\n",
    "            color='r')\n",
    "    max_count = max([discrimination['hist_obsved'].isel(lead_time=idx, drop=True).max(), \n",
    "                     discrimination['hist_not_obsved'].isel(lead_time=idx, drop=True).max()])\n",
    "    ax.text(0.9,0.85*max_count,'mn '+str(idx+1))\n",
    "    \n",
    "    if idx % ncol == 0:\n",
    "        ax.set_ylabel('Likelihood')\n",
    "        \n",
    "    if idx / ncol >= nrow - 1:\n",
    "        ax.set_xlabel('Forecast probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute discrimination diagram for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimination = verif.compute_discrimination(fcst_likelihood, obsv_logical, \n",
    "                                              fcst_probabilities, indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,3))\n",
    "\n",
    "ax1 = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax1.grid()\n",
    "scale_width = 2.5\n",
    "ax1.bar(discrimination.bins-discrimination.bins[1]/scale_width/2,\n",
    "        discrimination['hist_obsved'],\n",
    "        width=discrimination.bins[1]/scale_width,\n",
    "        color='b')\n",
    "ax1.bar(discrimination.bins+discrimination.bins[1]/scale_width/2,\n",
    "        discrimination['hist_not_obsved'],\n",
    "        width=discrimination.bins[1]/scale_width,\n",
    "        color='r')\n",
    "ax1.set_xlabel('Forecast probability')\n",
    "ax1.set_ylabel('Likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for categorized forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for 4 categories between 25 and 150 mm of monthly rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category edges -----\n",
    "category_edges = linspace(25,150,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute contingency as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contingency table -----\n",
    "contingency = verif.compute_contingency_table(da_fcst,da_obsv, category_edges,\n",
    "                                              ensemble_dim='ensemble', indep_dims=('init_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 4; nrow = int(ceil(len(lead_times)/ncol));\n",
    "fig, axs = plt.subplots(figsize=(10,15), nrows=nrow, ncols=ncol);\n",
    "\n",
    "for idx,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.grid()\n",
    "    im = ax.imshow(contingency.isel(lead_time=idx, drop=True))\n",
    "    # ax.text(0.82,0.7,'mn '+str(idx+1))\n",
    "    \n",
    "    if idx % ncol == 0:\n",
    "        ax.set_ylabel('Forecast category')\n",
    "        \n",
    "    if idx / ncol >= nrow - 1:\n",
    "        ax.set_xlabel('Observed category')\n",
    "        \n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax);\n",
    "cbar_ax.set_ylabel('counts', rotation=270, labelpad=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute contingency for all lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contingency table -----\n",
    "contingency = verif.compute_contingency_table(da_fcst,da_obsv,category_edges,\n",
    "                                              nsemble_dim='ensemble',indep_dims=('init_date','lead_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,4))\n",
    "\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "im = ax.imshow(contingency)\n",
    "ax.set_xlabel('Observed category')\n",
    "ax.set_ylabel('Forecast category')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.72, 0.15, 0.03, 0.7])\n",
    "fig.colorbar(im, cax=cbar_ax);\n",
    "cbar_ax.set_ylabel('counts', rotation=270, labelpad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute contingency as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contingency table -----\n",
    "contingency = verif.compute_contingency_table(da_fcst,da_obsv, category_edges,\n",
    "                                              ensemble_dim='ensemble', indep_dims=('init_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy_score = verif.compute_accuracy_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heidke skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heidke_score = verif.compute_Heidke_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peirce skill score / Hanssen and Kuipers discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Peirce_score = verif.compute_Peirce_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerrity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gerrity_score = verif.compute_Gerrity_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(accuracy_score['lead_time'],accuracy_score,linewidth=2)\n",
    "ax.plot(Heidke_score['lead_time'],Heidke_score,linewidth=2)\n",
    "ax.plot(Peirce_score['lead_time'],Peirce_score,linewidth=2)\n",
    "ax.plot(Gerrity_score['lead_time'],Gerrity_score,linewidth=2)\n",
    "ax.set_xlabel('Lead time [months]')\n",
    "ax.set_ylabel('Score');\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for dichotomously categorized forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E.g. for monthly rainfall being > or < 100 mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category edges -----\n",
    "category_edges = [-inf, 100, inf]\n",
    "\n",
    "# Compute contingency table -----\n",
    "contingency = verif.compute_contingency_table(da_fcst,da_obsv,category_edges,\n",
    "                                              ensemble_dim='ensemble',indep_dims=('init_date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_score = verif.compute_bias_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate = verif.compute_hit_rate(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarm_ratio = verif.compute_false_alarm_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False alarm rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_alarm_rate = verif.compute_false_alarm_rate(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_ratio = verif.compute_success_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_score = verif.compute_threat_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equitable threat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equit_threat_score = verif.compute_equit_threat_score(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio = verif.compute_odds_ratio(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds ratio skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio_skill = verif.compute_odds_ratio_skill(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(bias_score['lead_time'],bias_score,linewidth=2)\n",
    "ax.plot(hit_rate['lead_time'],hit_rate,linewidth=2)\n",
    "ax.plot(false_alarm_ratio['lead_time'],false_alarm_ratio,linewidth=2)\n",
    "ax.plot(false_alarm_rate['lead_time'],false_alarm_rate,linewidth=2)\n",
    "ax.plot(success_ratio['lead_time'],success_ratio,linewidth=2)\n",
    "ax.plot(threat_score['lead_time'],threat_score,linewidth=2)\n",
    "ax.plot(equit_threat_score['lead_time'],equit_threat_score,linewidth=2)\n",
    "# ax.plot(odds_ratio['lead_time'],odds_ratio,linewidth=2)\n",
    "ax.plot(odds_ratio_skill['lead_time'],odds_ratio_skill,linewidth=2)\n",
    "ax.set_xlabel('Lead time [months]')\n",
    "ax.set_ylabel('Score');\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive bias error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_additive_bias = verif.compute_mean_additive_bias(da_fcst, da_obsv, \n",
    "                                                      indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicative bias error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_multiplicative_bias = verif.compute_mean_multiplicative_bias(da_fcst, da_obsv, \n",
    "                                                                  indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error = verif.compute_mean_absolute_error(da_fcst, da_obsv, \n",
    "                                                        indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error = verif.compute_mean_squared_error(da_fcst, da_obsv, \n",
    "                                                      indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_error = verif.compute_rms_error(da_fcst, da_obsv, \n",
    "                                    indep_dims='init_date', ensemble_dim='ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot as a function of lead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax = fig1.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "ax.grid()\n",
    "ax.plot(mean_additive_bias['lead_time'],mean_additive_bias,linewidth=2)\n",
    "ax.plot(mean_multiplicative_bias['lead_time'],mean_multiplicative_bias,linewidth=2)\n",
    "ax.plot(mean_absolute_error['lead_time'],mean_absolute_error,linewidth=2)\n",
    "# ax.plot(mean_squared_error['lead_time'],mean_squared_error,linewidth=2)\n",
    "ax.plot(rms_error['lead_time'],rms_error,linewidth=2)\n",
    "ax.set_xlabel('Lead time [months]')\n",
    "ax.set_ylabel('Error');\n",
    "legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dts3_env]",
   "language": "python",
   "name": "conda-env-dts3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
